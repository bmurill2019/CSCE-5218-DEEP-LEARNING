{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dZiXu4eVKWUK",
    "outputId": "e48ffb8b-92da-44e6-a481-f0cf871332a7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the datasets with linux commands\n",
    "!wget http://huang.eng.unt.edu/CSCE-5218/train.dat\n",
    "!wget http://huang.eng.unt.edu/CSCE-5218/test.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 11244  100 11244    0     0  72227      0 --:--:-- --:--:-- --:--:-- 73012\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  2844  100  2844    0     0  15591      0 --:--:-- --:--:-- --:--:-- 15800\n"
     ]
    }
   ],
   "source": [
    "# get the datasets with windows commands\n",
    "!curl.exe --output train.dat http://huang.eng.unt.edu/CSCE-5218/train.dat\n",
    "!curl.exe --output test.dat http://huang.eng.unt.edu/CSCE-5218/test.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vYiZq0X2oB5t"
   },
   "source": [
    "# **CSCE 5218 / CSCE 4930 Deep Learning**\n",
    "\n",
    "# **HW1a The Perceptron** (20 pt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "cXAsP_lw3QwJ"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "\n",
    "# Corpus reader, all columns but the last one are coordinates;\n",
    "#   the last column is the label\n",
    "def read_data(file_name):\n",
    "    f = open(file_name, 'r')\n",
    "    data = []\n",
    "    # Discard header line\n",
    "    f.readline()\n",
    "    for instance in f.readlines():\n",
    "        if not re.search('\\t', instance): continue\n",
    "        instance = list(map(int, instance.strip().split('\\t')))\n",
    "        # Add a dummy input so that w0 becomes the bias\n",
    "        instance = [-1] + instance\n",
    "        data += [instance]\n",
    "    return data\n",
    "\n",
    "def dot_product(array1, array2):\n",
    "    #the dot product is an operation that multiplies the items that shares the same items\n",
    "    #and then add the results. it is useful to express an equation in its vectorized form\n",
    "    # You do not to write code like this, but get used to it\n",
    "    return sum([w * x for w, x in zip(array1, array2)])\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    #the sigmoid function is the activation function that saqushes the \n",
    "    #output of each neuron into the range [0,1], wich is good for dealing with probabilities\n",
    "    #and also this function is continuous.\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "\n",
    "# Accuracy = percent of correct predictions\n",
    "def get_accuracy(weights, instances):\n",
    "    # You do not to write code like this, but get used to it\n",
    "    correct = sum([1 if predict(weights, instance) == instance[-1] else 0\n",
    "                   for instance in instances])\n",
    "    return correct * 100 / len(instances)\n",
    "\n",
    "\n",
    "# Predict a new instance; this is the definition of the perceptron\n",
    "def predict(weights, instance):\n",
    "    if sigmoid(dot_product(weights, instance)) >= 0.5:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "# Train a perceptron with instances\n",
    "#   and hyperparameters lr (leearning rate) and epochs\n",
    "# The implementation comes from the definition of the perceptron\n",
    "# Training consists on fitting the parameters\n",
    "#   The parameters are the weights, that's the only thing training is responsible to fit\n",
    "#     (recall that w0 is the bias, and w1..wn are the weights for each coordinate)\n",
    "#   Hyperparameters (lr and epochs) are given to the training algorithm\n",
    "# We are updating weights in the opposite direction of the gradient of the error,\n",
    "#   so with a \"decent\" lr we are guaranteed to reduce the error after each iteration.\n",
    "def train_perceptron(instances, lr, epochs):\n",
    "    '''\n",
    "    Inputs:\n",
    "        instances: the input features\n",
    "        lr: the learning rate\n",
    "        epochs: number of iterations allowed.\n",
    "        '''\n",
    "    \n",
    "    # initialize weights\n",
    "    weights = [0] * (len(instances[0])-1)\n",
    "    # weights = [0, 0, 0, ...,  0]\n",
    "    \n",
    "    # iterations\n",
    "    while epochs > 0:\n",
    "        #for every sample in the training dataset (instances)\n",
    "        for instance in instances:\n",
    "            in_value = dot_product(weights, instance) #get the features*weights\n",
    "            output = sigmoid(in_value)#apply the activation function\n",
    "            error = instance[-1] - output#get the error (linear cost function)\n",
    "            for i in range(0, len(weights)):#backpropagate the error tu update the weights\n",
    "                # weightUpdated = lr*error*(the proportion that specific weight impacted in the final result)\n",
    "                weights[i] += lr * error * output * (1-output) * instance[i]\n",
    "\n",
    "        epochs -= 1 #update iteration\n",
    "        if epochs == 0:\n",
    "            break\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vGVmKzgG2Ium",
    "outputId": "884c328d-8011-413f-a265-c46841fb4893"
   },
   "outputs": [],
   "source": [
    "# Get the datasets\n",
    "!curl.exe --output train.dat http://huang.eng.unt.edu/CSCE-5218/train.dat\n",
    "!curl.exe --output test.dat http://huang.eng.unt.edu/CSCE-5218/test.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "50YvUza-BYQF",
    "outputId": "bd39e8af-e81e-42b3-e5a9-f5a87f0ad1ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#tr: 400, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n"
     ]
    }
   ],
   "source": [
    "instances_tr = read_data(\"train.dat\")\n",
    "instances_te = read_data(\"test.dat\")\n",
    "lr = 0.005\n",
    "epochs = 5\n",
    "weights = train_perceptron(instances_tr, lr, epochs)\n",
    "accuracy = get_accuracy(weights, instances_te)\n",
    "print(f\"#tr: {len(instances_tr):3}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
    "      f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBXkvaiQMohX"
   },
   "source": [
    "## Questions\n",
    "\n",
    "Answer the following questions. Include your implementation and the output for each question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YCQ6BEk1CBlr"
   },
   "source": [
    "\n",
    "\n",
    "### Question 1\n",
    "\n",
    "In `train_perceptron(instances, lr, epochs)`, we have the following code:\n",
    "```\n",
    "in_value = dot_product(weights, instance)\n",
    "output = sigmoid(in_value)\n",
    "error = instance[-1] - output\n",
    "```\n",
    "\n",
    "Why don't we have the following code snippet instead?\n",
    "```\n",
    "output = predict(weights, instance)\n",
    "error = instance[-1] - output\n",
    "```\n",
    "\n",
    "#### TODO Add your answer here (text only)\n",
    "Because the predict function applies a threshold value to return either 0 or 1. This makes the convergence more difficult since the outcomes may jump suddenly from 0 to 1, changing the values of the cost function in an underirable way so the task of updating the weights get more complicated. Thatâ€™s way we want to retain the outcome of the activation function (which is any value between 0 and 1) to smoothly approach the weights that get the minimum value in the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JU3c3m6YL2rK"
   },
   "source": [
    "### Question 2\n",
    "Train the perceptron with the following hyperparameters and calculate the accuracy with the test dataset.\n",
    "\n",
    "```\n",
    "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
    "num_epochs = [5, 10, 20, 50, 100]              # number of epochs\n",
    "lr = [0.005, 0.01, 0.05]              # learning rate\n",
    "```\n",
    "\n",
    "TODO Write your code below and include the output of your code.\n",
    "The output should look like the following:\n",
    "```\n",
    "# tr:  20, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "# tr:  20, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "# tr:  20, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "[and so on for all the combinations]\n",
    "```\n",
    "You will get different results with differet hyperparameters.\n",
    "\n",
    "#### TODO Add your answer here (code and output in the format above) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#tr:  20, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr:  20, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr:  20, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr:  20, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr:  20, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 69.0\n",
      "#tr:  20, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr:  20, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr:  20, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr:  20, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 69.0\n",
      "#tr:  20, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 73.0\n",
      "#tr:  20, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "#tr:  20, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 69.0\n",
      "#tr:  20, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 73.0\n",
      "#tr:  20, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 67.0\n",
      "#tr:  20, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 67.0\n",
      "#tr:  40, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr:  40, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr:  40, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr:  40, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr:  40, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr:  40, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr:  40, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr:  40, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr:  40, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr:  40, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 71.0\n",
      "#tr:  40, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "#tr:  40, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "#tr:  40, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 72.0\n",
      "#tr:  40, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 73.0\n",
      "#tr:  40, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 74.0\n",
      "#tr: 100, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 100, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 100, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 100, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 100, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 69.0\n",
      "#tr: 100, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 100, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 100, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 100, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 69.0\n",
      "#tr: 100, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 100, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 100, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 100, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 100, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 72.0\n",
      "#tr: 100, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 73.0\n",
      "#tr: 200, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 200, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 200, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 200, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 200, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 73.0\n",
      "#tr: 200, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 200, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 200, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 200, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 73.0\n",
      "#tr: 200, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 75.0\n",
      "#tr: 200, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 69.0\n",
      "#tr: 200, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 73.0\n",
      "#tr: 200, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 76.0\n",
      "#tr: 200, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 77.0\n",
      "#tr: 200, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 79.0\n",
      "#tr: 300, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 300, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 300, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 300, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 69.0\n",
      "#tr: 300, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 72.0\n",
      "#tr: 300, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 300, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 300, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 300, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 72.0\n",
      "#tr: 300, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 79.0\n",
      "#tr: 300, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 69.0\n",
      "#tr: 300, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 72.0\n",
      "#tr: 300, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 77.0\n",
      "#tr: 300, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 79.0\n",
      "#tr: 300, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 78.0\n",
      "#tr: 400, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 400, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 400, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 69.0\n",
      "#tr: 400, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 73.0\n",
      "#tr: 400, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 77.0\n",
      "#tr: 400, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 400, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 69.0\n",
      "#tr: 400, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 69.0\n",
      "#tr: 400, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 77.0\n",
      "#tr: 400, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 79.0\n",
      "#tr: 400, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 74.0\n",
      "#tr: 400, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 77.0\n",
      "#tr: 400, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 79.0\n",
      "#tr: 400, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 80.0\n",
      "#tr: 400, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 80.0\n"
     ]
    }
   ],
   "source": [
    "# with random sampling\n",
    "import random\n",
    "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
    "num_epochs = [5, 10, 20, 50, 100]              # number of epochs\n",
    "lr = [0.005, 0.01, 0.05]              # learning rate\n",
    "\n",
    "for tr_ in tr_percent: \n",
    "    # sample without replacement\n",
    "    random.seed(tr_)\n",
    "    trainingBatch = random.sample(instances_tr, k = int(tr_/100*len(instances_tr)))\n",
    "    for lr_ in lr:\n",
    "        for epochs_ in num_epochs:\n",
    "            weights = train_perceptron(trainingBatch, lr_, epochs_)\n",
    "            accuracy = get_accuracy(weights, instances_te)\n",
    "            print(f\"#tr: {len(trainingBatch):3}, epochs: {epochs_:3}, learning rate: {lr_:.3f}; \"\n",
    "                  f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#tr:  20, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr:  20, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr:  20, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr:  20, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr:  20, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr:  20, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr:  20, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr:  20, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr:  20, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr:  20, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr:  20, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "#tr:  20, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "#tr:  20, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "#tr:  20, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "#tr:  20, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 64.0\n",
      "#tr:  40, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr:  40, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr:  40, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr:  40, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr:  40, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr:  40, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr:  40, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr:  40, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr:  40, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr:  40, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr:  40, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "#tr:  40, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "#tr:  40, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "#tr:  40, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 71.0\n",
      "#tr:  40, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 69.0\n",
      "#tr: 100, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 100, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 100, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 100, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 100, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 100, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 100, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 100, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 100, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 100, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 71.0\n",
      "#tr: 100, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 100, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 67.0\n",
      "#tr: 100, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 70.0\n",
      "#tr: 100, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 74.0\n",
      "#tr: 100, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 77.0\n",
      "#tr: 200, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 200, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 200, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 200, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 67.0\n",
      "#tr: 200, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 74.0\n",
      "#tr: 200, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 200, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 200, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 200, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 74.0\n",
      "#tr: 200, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 78.0\n",
      "#tr: 200, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 71.0\n",
      "#tr: 200, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 77.0\n",
      "#tr: 200, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 78.0\n",
      "#tr: 200, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 78.0\n",
      "#tr: 200, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 76.0\n",
      "#tr: 300, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 300, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 300, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 300, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 74.0\n",
      "#tr: 300, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 78.0\n",
      "#tr: 300, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 300, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 300, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 70.0\n",
      "#tr: 300, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 78.0\n",
      "#tr: 300, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 80.0\n",
      "#tr: 300, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 74.0\n",
      "#tr: 300, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 78.0\n",
      "#tr: 300, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 79.0\n",
      "#tr: 300, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 78.0\n",
      "#tr: 300, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 77.0\n",
      "#tr: 400, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 400, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 400, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 69.0\n",
      "#tr: 400, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 73.0\n",
      "#tr: 400, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 77.0\n",
      "#tr: 400, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 400, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 69.0\n",
      "#tr: 400, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 70.0\n",
      "#tr: 400, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 77.0\n",
      "#tr: 400, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 80.0\n",
      "#tr: 400, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 69.0\n",
      "#tr: 400, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 76.0\n",
      "#tr: 400, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 80.0\n",
      "#tr: 400, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 80.0\n",
      "#tr: 400, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 80.0\n"
     ]
    }
   ],
   "source": [
    "# without random sampling\n",
    "\n",
    "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
    "num_epochs = [5, 10, 20, 50, 100]              # number of epochs\n",
    "lr = [0.005, 0.01, 0.05]              # learning rate\n",
    "\n",
    "for tr_ in tr_percent: \n",
    "    trainingBatch = instances_tr[0:int(tr_/100*len(instances_tr))]\n",
    "    for lr_ in lr:\n",
    "        for epochs_ in num_epochs:\n",
    "            weights = train_perceptron(trainingBatch, lr_, epochs_)\n",
    "            accuracy = get_accuracy(weights, instances_te)\n",
    "            print(f\"#tr: {len(trainingBatch):3}, epochs: {epochs_:3}, learning rate: {lr_:.3f}; \"\n",
    "                  f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFB9MtwML24O"
   },
   "source": [
    "### Question 3\n",
    "Write a couple paragraphs interpreting the results with all the combinations of hyperparameters. Drawing a plot will probably help you make a point. In particular, answer the following:\n",
    "1. Do you need to train with all the training dataset to get the highest accuracy with the test dataset?\n",
    "2. How do you justify that training the second run obtains worse accuracy than the first one (despite the second one uses more training data)?\n",
    "```\n",
    "#tr: 100, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 71.0\n",
    "#tr: 200, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "```\n",
    "3. Can you get higher accuracy with additional hyperparameters (higher than `80.0`)?\n",
    "4. Is it always worth training for more epochs (while keeping all other hyperparameters fixed)?\n",
    "\n",
    "#### TODO Add your answer here (code and text)\n",
    "1. As it can be seen in the iteration\n",
    "```\n",
    "#tr: 300, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 80.0\n",
    "```\n",
    "    it was not needed the whole training dataset to achieve the accuracy of 80.0, this means that it's performance also depends on the values of the hyperparameters (Lr and epochs). it can be also possible that a smaller training set generalizes well the data so the unseen data may have better results.\n",
    "2. I'd say that the learning rate in the second run is so small that does not allow the model to move a lot in the cost function curve, it means that the steps are small so more epochs are needed to reach the minimum. Whereas the learning rate for the first run was higher, it allows the model to make bigger steps and be closer to the minimum.\n",
    "3. the accuracy of 80.0 may be related to the shape of the loss function (where the minimum is located) so trying with a different one may increase the accuracy, also introducing a Regularization term to the update equation may make the model generalize better when unseen data is provided.\n",
    "4. No, it is not. looking at the last 3 outcomes, we can see that the model has reached a steady state, so more than 20 epochs were not required in this case. So, in order to search for a better result, one solution may be making big steps at the beginning (big Lr) and then small steps that avoid the model keeps jumping a possible better minimum value for the cost function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pending Write code to plot outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
